{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71e154df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory path\n",
    "data_dir = Path(\"../aligned_timeseries_FINS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42805d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all CSV files matching the pattern\n",
    "matching_files = [f for f in data_dir.glob(\"FINS_*c_*_filtered.csv\")]\n",
    "\n",
    "# List to store files with post_chat\n",
    "files_with_post_chat = []\n",
    "\n",
    "# Check each file for post_chat in condition column\n",
    "for file_path in matching_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'condition' in df.columns:\n",
    "            if 'post_chat' in df['condition'].values:\n",
    "                files_with_post_chat.append(file_path.name)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path.name}: {e}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Total matching files: {len(matching_files)}\")\n",
    "print(f\"Files with 'post_chat': {len(files_with_post_chat)}\")\n",
    "print(\"\\nFiles containing 'post_chat' in condition column:\")\n",
    "for filename in sorted(files_with_post_chat):\n",
    "    print(f\"  - {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6b10fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv('../aligned_timeseries_FINS/FINS_056c_aligned_timeseries_filtered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5130ccf",
   "metadata": {},
   "source": [
    "## Plan for fitting two independent HMMs (child1 HMM, child2 HMM), then compare state sequences\n",
    "\n",
    "### Step 1 — Load both CSVs\n",
    "\n",
    "### Step 2 — Filter to `post_chat`\n",
    "\n",
    "### Step 3 — Select HbO columns only → build X\n",
    "\n",
    "### Step 4 — EDA on filtered data\n",
    "\n",
    "* duration (sec/min)\n",
    "* sampling rate (~10 Hz)\n",
    "* missingness rate (fraction of NaNs)\n",
    "* number of HbO channels\n",
    "\n",
    "### Step 5 — Standardize X (mean 0, std 1 per HbO channel)\n",
    "\n",
    "### Step 6 — Fit HMM on each file (same K)\n",
    "\n",
    "* start with K = 4..10\n",
    "* pick K later by BIC/stability; for now pick a reasonable K like 6\n",
    "\n",
    "### Step 7 — Align state labels across the two HMMs (Hungarian)\n",
    "\n",
    "* align by similarity of state mean patterns\n",
    "\n",
    "### Step 8 — Compare aligned outputs\n",
    "\n",
    "* fractional occupancy per aligned state\n",
    "* dwell time per aligned state\n",
    "\n",
    "Focused on:\n",
    "\n",
    "* `condition == \"post_chat\"`\n",
    "* **HbO only**\n",
    "* EDA: minutes, sampling rate, missingness\n",
    "* standardization (recommended for HMM stability)\n",
    "* fit HMM to each file\n",
    "* align states between the two HMMs using **Hungarian algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd2abe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip -q install hmmlearn scikit-learn scipy pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d05777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b40b92",
   "metadata": {},
   "source": [
    "### EDA Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df56adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hbo_columns(df):\n",
    "    # Matches columns ending with ' hbo' (case-insensitive)\n",
    "    return [c for c in df.columns if c.lower().endswith(\" hbo\")]\n",
    "\n",
    "def missingness_rate(X):\n",
    "    # fraction of NaNs in the whole matrix\n",
    "    return float(np.isnan(X).mean())\n",
    "\n",
    "def eda_postchat_hbo(df, name=\"file\"):\n",
    "    \"\"\"\n",
    "    Filter to post_chat condition, extract HbO columns, and report EDA stats.\n",
    "    Assumes 10 Hz sampling rate.\n",
    "    \"\"\"\n",
    "    if \"condition\" not in df.columns:\n",
    "        raise ValueError(f\"{name}: no 'condition' column found.\")\n",
    "    \n",
    "    # Show available conditions for debugging\n",
    "    print(f\"\\nAvailable conditions in {name}: {list(pd.unique(df['condition']))}\")\n",
    "    \n",
    "    # Filter to post_chat\n",
    "    d = df[df[\"condition\"] == \"post_chat\"].copy()\n",
    "    \n",
    "    if len(d) == 0:\n",
    "        raise ValueError(f\"{name}: no rows found with condition == 'post_chat'.\")\n",
    "    \n",
    "    # Get HbO columns\n",
    "    hbo_cols = get_hbo_columns(d)\n",
    "    if len(hbo_cols) == 0:\n",
    "        raise ValueError(f\"{name}: no HbO columns found after filtering to post_chat.\")\n",
    "    \n",
    "    # Extract data\n",
    "    X = d[hbo_cols].astype(float).values\n",
    "    \n",
    "    # Calculate duration from row count (10 Hz sampling)\n",
    "    sampling_hz = 10.0\n",
    "    duration_sec = len(d) / sampling_hz\n",
    "    duration_min = duration_sec / 60.0\n",
    "    \n",
    "    # Calculate missingness\n",
    "    miss = missingness_rate(X)\n",
    "    \n",
    "    # Print EDA summary\n",
    "    print(f\"\\n=== EDA (post_chat, HbO only): {name} ===\")\n",
    "    print(f\"Total rows in file: {len(df)}\")\n",
    "    print(f\"Post_chat rows: {len(d)}\")\n",
    "    print(f\"Percentage: {100 * len(d) / len(df):.1f}%\")\n",
    "    print(f\"HbO channels (D): {len(hbo_cols)}\")\n",
    "    print(f\"Duration: {duration_sec:.2f} sec (~{duration_min:.2f} min)\")\n",
    "    print(f\"Missingness rate (NaN fraction): {miss:.6f}\")\n",
    "    \n",
    "    # Return time column for compatibility (though not used downstream)\n",
    "    t = d[\"time_sec\"].values if \"time_sec\" in d.columns else np.arange(len(d)) / sampling_hz\n",
    "    \n",
    "    return d, hbo_cols, X, t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9cf2e",
   "metadata": {},
   "source": [
    "### Loading files + run post_chat HbO EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbbb7b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available conditions in FINS_056c: ['pre_chat', 'easy_talking', 'easy_silence', 'med_talking', 'med_silence', 'hard_talking', 'hard_silence', 'post_chat']\n",
      "\n",
      "=== EDA (post_chat, HbO only): FINS_056c ===\n",
      "Total rows in file: 5679\n",
      "Post_chat rows: 799\n",
      "Percentage: 14.1%\n",
      "HbO channels (D): 22\n",
      "Duration: 79.90 sec (~1.33 min)\n",
      "Missingness rate (NaN fraction): 0.000000\n",
      "\n",
      "Available conditions in FINS_057c: ['pre_chat', 'easy_talking', 'easy_silence', 'med_talking', 'med_silence', 'hard_talking', 'hard_silence', 'post_chat']\n",
      "\n",
      "=== EDA (post_chat, HbO only): FINS_057c ===\n",
      "Total rows in file: 5699\n",
      "Post_chat rows: 688\n",
      "Percentage: 12.1%\n",
      "HbO channels (D): 22\n",
      "Duration: 68.80 sec (~1.15 min)\n",
      "Missingness rate (NaN fraction): 0.000000\n",
      "\n",
      "Common HbO channels: 22\n"
     ]
    }
   ],
   "source": [
    "file_a = f\"{data_dir}/FINS_056c_aligned_timeseries_filtered.csv\"\n",
    "file_b = f\"{data_dir}/FINS_057c_aligned_timeseries_filtered.csv\"\n",
    "\n",
    "df_a = pd.read_csv(file_a)\n",
    "df_b = pd.read_csv(file_b)\n",
    "\n",
    "d_a, hbo_a, Xa, ta = eda_postchat_hbo(df_a, name=\"FINS_056c\")\n",
    "d_b, hbo_b, Xb, tb = eda_postchat_hbo(df_b, name=\"FINS_057c\")\n",
    "\n",
    "# Ensure we use the same HbO channels in both (intersection if needed)\n",
    "common_hbo = sorted(list(set(hbo_a).intersection(set(hbo_b))))\n",
    "print(\"\\nCommon HbO channels:\", len(common_hbo))\n",
    "\n",
    "Xa = d_a[common_hbo].astype(float).values\n",
    "Xb = d_b[common_hbo].astype(float).values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809550c4",
   "metadata": {},
   "source": [
    "### Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfe968cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    scaler = StandardScaler()\n",
    "    Xz = scaler.fit_transform(X)\n",
    "    return Xz, scaler\n",
    "\n",
    "Xa_z, scaler_a = standardize(Xa)\n",
    "Xb_z, scaler_b = standardize(Xb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29247ef8",
   "metadata": {},
   "source": [
    "### Fit HMM (no PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f02db12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_hmm(Xz, K, seed=0, n_iter=500):\n",
    "    model = GaussianHMM(\n",
    "        n_components=K,\n",
    "        covariance_type=\"diag\",\n",
    "        n_iter=n_iter,\n",
    "        tol=1e-3,\n",
    "        random_state=seed\n",
    "    )\n",
    "    model.fit(Xz)\n",
    "    z = model.predict(Xz)\n",
    "    return model, z\n",
    "\n",
    "K = 6  # starter value; we can sweep K later\n",
    "\n",
    "model_a, z_a = fit_hmm(Xa_z, K=K, seed=0)\n",
    "model_b, z_b = fit_hmm(Xb_z, K=K, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bc0495",
   "metadata": {},
   "source": [
    "### Hungarian alignment (align B’s states to A’s labels)\n",
    "We align using cosine similarity of the state mean vectors (`model.means_`), which are in the standardized feature space here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e131b3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping (A_state -> B_state): [4, 5, 3, 2, 1, 0]\n",
      "Avg matched similarity: 0.27999338086159187\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity_matrix(A, B, eps=1e-9):\n",
    "    A_norm = A / (np.linalg.norm(A, axis=1, keepdims=True) + eps)\n",
    "    B_norm = B / (np.linalg.norm(B, axis=1, keepdims=True) + eps)\n",
    "    return A_norm @ B_norm.T\n",
    "\n",
    "def hungarian_align(means_a, means_b):\n",
    "    S = cosine_similarity_matrix(means_a, means_b)\n",
    "    cost = -S  # maximize similarity\n",
    "    row_ind, col_ind = linear_sum_assignment(cost)\n",
    "    # col_ind[a_state] = matched b_state\n",
    "    return col_ind, S\n",
    "\n",
    "def remap_states_b_to_a(z_b, mapping_a_to_b):\n",
    "    # Build inverse map: inv[b_state] = a_state\n",
    "    inv = np.zeros_like(mapping_a_to_b)\n",
    "    for a_state, b_state in enumerate(mapping_a_to_b):\n",
    "        inv[b_state] = a_state\n",
    "    return inv[z_b]\n",
    "\n",
    "mapping_a_to_b, sim = hungarian_align(model_a.means_, model_b.means_)\n",
    "z_b_aligned = remap_states_b_to_a(z_b, mapping_a_to_b)\n",
    "\n",
    "print(\"Mapping (A_state -> B_state):\", mapping_a_to_b.tolist())\n",
    "print(\"Avg matched similarity:\", float(np.mean([sim[a, mapping_a_to_b[a]] for a in range(K)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2fb228",
   "metadata": {},
   "source": [
    "### Compare aligned occupancy + dwell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1b47aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FO A: [0.218 0.242 0.081 0.091 0.215 0.153]\n",
      "FO B (aligned): [0.113 0.215 0.126 0.188 0.166 0.192]\n",
      "Mean dwell A (steps): [ 87.   96.5  32.5  24.3  57.3 122. ]\n",
      "Mean dwell B (aligned, steps): [ 26.  148.   87.   21.5  28.5  22. ]\n"
     ]
    }
   ],
   "source": [
    "def fractional_occupancy(z, K):\n",
    "    return np.bincount(z, minlength=K) / len(z)\n",
    "\n",
    "def mean_dwell(z, K):\n",
    "    dw = {k: [] for k in range(K)}\n",
    "    s, run = z[0], 1\n",
    "    for i in range(1, len(z)):\n",
    "        if z[i] == s:\n",
    "            run += 1\n",
    "        else:\n",
    "            dw[s].append(run)\n",
    "            s, run = z[i], 1\n",
    "    dw[s].append(run)\n",
    "    return np.array([np.mean(dw[k]) if dw[k] else 0.0 for k in range(K)])\n",
    "\n",
    "fo_a = fractional_occupancy(z_a, K)\n",
    "fo_b = fractional_occupancy(z_b_aligned, K)\n",
    "\n",
    "dt_a = mean_dwell(z_a, K)\n",
    "dt_b = mean_dwell(z_b_aligned, K)\n",
    "\n",
    "print(\"\\nFO A:\", np.round(fo_a, 3))\n",
    "print(\"FO B (aligned):\", np.round(fo_b, 3))\n",
    "print(\"Mean dwell A (steps):\", np.round(dt_a, 1))\n",
    "print(\"Mean dwell B (aligned, steps):\", np.round(dt_b, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4aee31",
   "metadata": {},
   "source": [
    "## HMM Tryout Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c45d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers: selection / loading\n",
    "# ----------------------------\n",
    "def load_postchat_hbo_matrix(csv_path: Path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      subject_id (str or None), T (int), hbo_cols (list[str]), X (np.ndarray shape T x D)\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if \"condition\" not in df.columns:\n",
    "        raise ValueError(f\"{csv_path.name}: no 'condition' column\")\n",
    "\n",
    "    d = df[df[\"condition\"] == \"post_chat\"].copy()\n",
    "    hbo_cols = get_hbo_columns(d)\n",
    "    if len(hbo_cols) == 0:\n",
    "        raise ValueError(f\"{csv_path.name}: no HbO columns found\")\n",
    "\n",
    "    X = d[hbo_cols].astype(float).to_numpy()\n",
    "    subject_id = None\n",
    "    if \"subject_id\" in d.columns and len(d) > 0:\n",
    "        subject_id = str(d[\"subject_id\"].iloc[0])\n",
    "\n",
    "    return subject_id, len(d), hbo_cols, X\n",
    "\n",
    "def zscore_within_subject(X: np.ndarray):\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    return scaler.fit_transform(X)\n",
    "\n",
    "# ----------------------------\n",
    "# Metrics from decoded states\n",
    "# ----------------------------\n",
    "def fractional_occupancy(z: np.ndarray, K: int):\n",
    "    fo = np.bincount(z, minlength=K).astype(float)\n",
    "    fo /= max(len(z), 1)\n",
    "    return fo\n",
    "\n",
    "def dwell_times_seconds(z: np.ndarray, fs: float, K: int):\n",
    "    \"\"\"\n",
    "    Mean dwell time per state in seconds.\n",
    "    \"\"\"\n",
    "    if len(z) == 0:\n",
    "        return np.full(K, np.nan)\n",
    "\n",
    "    dwell_lists = [[] for _ in range(K)]\n",
    "    run_state = z[0]\n",
    "    run_len = 1\n",
    "    for s in z[1:]:\n",
    "        if s == run_state:\n",
    "            run_len += 1\n",
    "        else:\n",
    "            dwell_lists[run_state].append(run_len)\n",
    "            run_state = s\n",
    "            run_len = 1\n",
    "    dwell_lists[run_state].append(run_len)\n",
    "\n",
    "    means = []\n",
    "    for k in range(K):\n",
    "        if len(dwell_lists[k]) == 0:\n",
    "            means.append(np.nan)\n",
    "        else:\n",
    "            means.append(np.mean(dwell_lists[k]) / fs)\n",
    "    return np.array(means, dtype=float)\n",
    "\n",
    "def empirical_transition_matrix(z: np.ndarray, K: int):\n",
    "    \"\"\"\n",
    "    Row-normalized empirical transitions from decoded state sequence z.\n",
    "    \"\"\"\n",
    "    A = np.zeros((K, K), dtype=float)\n",
    "    if len(z) < 2:\n",
    "        return A\n",
    "    for a, b in zip(z[:-1], z[1:]):\n",
    "        A[a, b] += 1.0\n",
    "    row_sums = A.sum(axis=1, keepdims=True)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        A = np.divide(A, row_sums, out=np.zeros_like(A), where=row_sums > 0)\n",
    "    return A\n",
    "\n",
    "# ----------------------------\n",
    "# Hungarian alignment + similarity\n",
    "# ----------------------------\n",
    "def cosine_similarity_matrix(M1: np.ndarray, M2: np.ndarray):\n",
    "    \"\"\"\n",
    "    M1: K x D, M2: K x D\n",
    "    returns K x K similarity matrix\n",
    "    \"\"\"\n",
    "    K1, D1 = M1.shape\n",
    "    K2, D2 = M2.shape\n",
    "    assert D1 == D2\n",
    "\n",
    "    # normalize rows\n",
    "    M1n = M1 / (norm(M1, axis=1, keepdims=True) + 1e-12)\n",
    "    M2n = M2 / (norm(M2, axis=1, keepdims=True) + 1e-12)\n",
    "    return M1n @ M2n.T  # K1 x K2\n",
    "\n",
    "def hungarian_match_means(means_ref: np.ndarray, means_other: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      perm: array length K mapping ref_state -> other_state\n",
    "      avg_sim: average cosine similarity after matching\n",
    "      sim_mat: KxK similarity matrix\n",
    "    \"\"\"\n",
    "    sim = cosine_similarity_matrix(means_ref, means_other)  # maximize\n",
    "    cost = -sim\n",
    "    r, c = linear_sum_assignment(cost)\n",
    "    perm = c[np.argsort(r)]  # ensure in ref state order\n",
    "    avg_sim = sim[np.arange(sim.shape[0]), perm].mean()\n",
    "    return perm, float(avg_sim), sim\n",
    "\n",
    "# ----------------------------\n",
    "# HMM fitting per seed\n",
    "# ----------------------------\n",
    "def fit_gaussian_hmm(X: np.ndarray, K: int, seed: int, n_iter: int = 300):\n",
    "    model = GaussianHMM(\n",
    "        n_components=K,\n",
    "        covariance_type=\"diag\",\n",
    "        n_iter=n_iter,\n",
    "        tol=1e-3,\n",
    "        random_state=seed,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(X)\n",
    "    logL = model.score(X)\n",
    "    z = model.predict(X)\n",
    "    return model, float(logL), z\n",
    "\n",
    "# ----------------------------\n",
    "# Stability across seeds (within subject)\n",
    "# ----------------------------\n",
    "def stability_across_seeds(models_means: list[np.ndarray]):\n",
    "    \"\"\"\n",
    "    Compute average similarity across seed runs by aligning each run to the best run.\n",
    "    Returns dict with mean/min similarities and the per-run similarities.\n",
    "    \"\"\"\n",
    "    # pick reference as first means (caller will pass best first), or use best externally\n",
    "    ref = models_means[0]\n",
    "    sims = []\n",
    "    for m in models_means[1:]:\n",
    "        _, avg_sim, _ = hungarian_match_means(ref, m)\n",
    "        sims.append(avg_sim)\n",
    "    if len(sims) == 0:\n",
    "        return {\"stability_mean\": np.nan, \"stability_min\": np.nan, \"stability_list\": []}\n",
    "    return {\n",
    "        \"stability_mean\": float(np.mean(sims)),\n",
    "        \"stability_min\": float(np.min(sims)),\n",
    "        \"stability_list\": sims,\n",
    "    }\n",
    "\n",
    "# ----------------------------\n",
    "# Baseline A runner\n",
    "# ----------------------------\n",
    "def run_baseline_A_individual(\n",
    "    data_dir: Path,\n",
    "    pattern: str = \"FINS_*c_aligned_timeseries_filtered.csv\",\n",
    "    K: int = 3,\n",
    "    seeds: list[int] = None,\n",
    "    fs: float = 10.0,\n",
    "    min_seconds: float = 45.0,\n",
    "    n_iter: int = 300,\n",
    "    output_dir: Path = None\n",
    "):\n",
    "    if seeds is None:\n",
    "        seeds = list(range(10))\n",
    "    min_T = int(min_seconds * fs)\n",
    "\n",
    "    csv_paths = sorted(data_dir.glob(pattern))\n",
    "    if len(csv_paths) == 0:\n",
    "        raise ValueError(f\"No files matched {pattern} in {data_dir}\")\n",
    "\n",
    "    if output_dir is None:\n",
    "        output_dir = data_dir / \"baselineA_outputs\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    rows_summary = []\n",
    "    per_subject_state_means = {}  # subject -> best means KxD\n",
    "\n",
    "    for p in csv_paths:\n",
    "        subject_id, T, hbo_cols, X = load_postchat_hbo_matrix(p)\n",
    "\n",
    "        # identify child files if role exists; otherwise run all \"*c*\" is already child by name\n",
    "        # (you can add a role check here later if needed)\n",
    "\n",
    "        if T < min_T:\n",
    "            rows_summary.append({\n",
    "                \"file\": p.name,\n",
    "                \"subject_id\": subject_id,\n",
    "                \"T\": T,\n",
    "                \"seconds\": T / fs,\n",
    "                \"included\": False,\n",
    "                \"reason\": f\"T<{min_T} (<{min_seconds}s)\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # standardize within subject\n",
    "        Xs = zscore_within_subject(X)\n",
    "\n",
    "        # fit multiple seeds\n",
    "        seed_results = []\n",
    "        for s in seeds:\n",
    "            try:\n",
    "                model, logL, z = fit_gaussian_hmm(Xs, K=K, seed=s, n_iter=n_iter)\n",
    "                seed_results.append((s, logL, model, z))\n",
    "            except Exception as e:\n",
    "                seed_results.append((s, -np.inf, None, None))\n",
    "\n",
    "        # pick best run by log-likelihood\n",
    "        seed_results_sorted = sorted(seed_results, key=lambda x: x[1], reverse=True)\n",
    "        best_seed, best_logL, best_model, best_z = seed_results_sorted[0]\n",
    "\n",
    "        if best_model is None or best_z is None or not np.isfinite(best_logL):\n",
    "            rows_summary.append({\n",
    "                \"file\": p.name,\n",
    "                \"subject_id\": subject_id,\n",
    "                \"T\": T,\n",
    "                \"seconds\": T / fs,\n",
    "                \"included\": False,\n",
    "                \"reason\": \"all seeds failed\",\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # metrics for best run\n",
    "        fo = fractional_occupancy(best_z, K)\n",
    "        dwell_sec = dwell_times_seconds(best_z, fs=fs, K=K)\n",
    "        A_emp = empirical_transition_matrix(best_z, K)\n",
    "\n",
    "        # stability: align each run to the best run's state means\n",
    "        # gather means for successful runs, with best first\n",
    "        means_best = best_model.means_\n",
    "        means_list = [means_best]\n",
    "        for (s, logL, model, z) in seed_results_sorted[1:]:\n",
    "            if model is not None and np.isfinite(logL):\n",
    "                means_list.append(model.means_)\n",
    "        stab = stability_across_seeds(means_list)\n",
    "\n",
    "        # record summary row\n",
    "        row = {\n",
    "            \"file\": p.name,\n",
    "            \"subject_id\": subject_id,\n",
    "            \"T\": T,\n",
    "            \"seconds\": T / fs,\n",
    "            \"included\": True,\n",
    "            \"K\": K,\n",
    "            \"best_seed\": best_seed,\n",
    "            \"best_logL\": best_logL,\n",
    "            \"logL_mean\": float(np.mean([x[1] for x in seed_results if np.isfinite(x[1])])),\n",
    "            \"logL_std\": float(np.std([x[1] for x in seed_results if np.isfinite(x[1])], ddof=0)),\n",
    "            \"stability_mean\": stab[\"stability_mean\"],\n",
    "            \"stability_min\": stab[\"stability_min\"],\n",
    "        }\n",
    "\n",
    "        # add FO and dwell per state as separate columns\n",
    "        for k in range(K):\n",
    "            row[f\"FO_s{k}\"] = float(fo[k])\n",
    "            row[f\"dwell_s{k}_sec\"] = float(dwell_sec[k]) if np.isfinite(dwell_sec[k]) else np.nan\n",
    "\n",
    "        rows_summary.append(row)\n",
    "\n",
    "        # save per-subject artifacts\n",
    "        subj_key = subject_id if subject_id is not None else p.stem\n",
    "        per_subject_state_means[subj_key] = means_best\n",
    "\n",
    "        # transition matrix\n",
    "        A_df = pd.DataFrame(A_emp, columns=[f\"s{j}\" for j in range(K)], index=[f\"s{i}\" for i in range(K)])\n",
    "        A_df.to_csv(output_dir / f\"{subj_key}_K{K}_transitions_empirical.csv\")\n",
    "\n",
    "        # decoded states (optional, useful for later)\n",
    "        states_df = pd.DataFrame({\"state\": best_z})\n",
    "        states_df.to_csv(output_dir / f\"{subj_key}_K{K}_decoded_states.csv\", index=False)\n",
    "\n",
    "    # write master summary\n",
    "    summary_df = pd.DataFrame(rows_summary)\n",
    "    summary_path = output_dir / f\"baselineA_individual_summary_K{K}.csv\"\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "    return summary_df, per_subject_state_means, summary_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de537f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: -4002.447418273866 is not greater than -4002.447416724521. Delta is -1.5493446881009731e-06\n",
      "Model is not converging.  Current: -4002.4474182738604 is not greater than -4002.447416723286. Delta is -1.5505743249377701e-06\n",
      "Model is not converging.  Current: -4002.4474182738722 is not greater than -4002.447416723276. Delta is -1.5505961528106127e-06\n",
      "Model is not converging.  Current: -4002.447418273859 is not greater than -4002.447416723513. Delta is -1.5503460417676251e-06\n",
      "Model is not converging.  Current: -4002.4474182738627 is not greater than -4002.4474167232643. Delta is -1.5505984265473671e-06\n",
      "Model is not converging.  Current: -9986.403756858894 is not greater than -9986.40374199816. Delta is -1.486073415435385e-05\n",
      "Model is not converging.  Current: -9986.403809619793 is not greater than -9986.403759144814. Delta is -5.047497870691586e-05\n",
      "Model is not converging.  Current: -8892.170466512895 is not greater than -8892.170220609867. Delta is -0.00024590302746219095\n",
      "Model is not converging.  Current: -8892.170389866062 is not greater than -8892.17028797057. Delta is -0.00010189549175265711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ../aligned_timeseries_FINS/baselineA_outputs/baselineA_individual_summary_K3.csv\n",
      "                                        file subject_id     T  seconds  \\\n",
      "0  FINS_001c_aligned_timeseries_filtered.csv       None     0      0.0   \n",
      "1  FINS_004c_aligned_timeseries_filtered.csv       004c   671     67.1   \n",
      "2  FINS_006c_aligned_timeseries_filtered.csv       006c   718     71.8   \n",
      "3  FINS_007c_aligned_timeseries_filtered.csv       007c   736     73.6   \n",
      "4  FINS_008c_aligned_timeseries_filtered.csv       008c  1220    122.0   \n",
      "\n",
      "   included          reason    K  best_seed     best_logL     logL_mean  \\\n",
      "0     False  T<450 (<45.0s)  NaN        NaN           NaN           NaN   \n",
      "1      True             NaN  3.0        4.0 -14379.065612 -14379.065618   \n",
      "2      True             NaN  3.0        1.0 -16923.980814 -16937.660198   \n",
      "3      True             NaN  3.0        0.0 -15386.537234 -15438.796410   \n",
      "4      True             NaN  3.0        9.0 -29461.374691 -29769.187958   \n",
      "\n",
      "     logL_std  stability_mean  stability_min     FO_s0  dwell_s0_sec  \\\n",
      "0         NaN             NaN            NaN       NaN           NaN   \n",
      "1    0.000005        1.000000       1.000000  0.444113     29.800000   \n",
      "2    9.160722        0.991982       0.970245  0.352368      2.811111   \n",
      "3   34.211661        0.995970       0.994818  0.538043      7.920000   \n",
      "4  153.906729        0.977511       0.974502  0.275410      1.768421   \n",
      "\n",
      "      FO_s1  dwell_s1_sec     FO_s2  dwell_s2_sec  \n",
      "0       NaN           NaN       NaN           NaN  \n",
      "1  0.156483     10.500000  0.399404     13.400000  \n",
      "2  0.298050     21.400000  0.349582      3.137500  \n",
      "3  0.293478      5.400000  0.168478     12.400000  \n",
      "4  0.652459      4.682353  0.072131      0.733333  \n"
     ]
    }
   ],
   "source": [
    "summary_df, means_dict, summary_path = run_baseline_A_individual(\n",
    "    data_dir=data_dir,\n",
    "    K=3,\n",
    "    seeds=list(range(10)),\n",
    "    fs=10.0,\n",
    "    min_seconds=45.0,\n",
    "    n_iter=300\n",
    ")\n",
    "\n",
    "print(\"Wrote:\", summary_path)\n",
    "print(summary_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9b155",
   "metadata": {},
   "source": [
    "# Debug and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3291fa15",
   "metadata": {},
   "source": [
    "### DEBUG PER FILE post_chat Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fde8d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pre_chat' 'easy_talking' 'easy_silence' 'med_talking' 'med_silence'\n",
      " 'hard_talking' 'hard_silence' 'post_chat']\n"
     ]
    }
   ],
   "source": [
    "file_a = f\"{data_dir}/FINS_040c_aligned_timeseries_filtered.csv\"\n",
    "# # file_a = f\"{data_dir}/FINS_056c_aligned_timeseries_filtered.csv\"\n",
    "# # file_b = f\"{data_dir}/FINS_057c_aligned_timeseries_filtered.csv\"\n",
    "\n",
    "df_a = pd.read_csv(file_a)\n",
    "print(pd.unique(df_a['condition']))\n",
    "\n",
    "# def eda_postchat_hbo(df, name=\"file\"):\n",
    "#     if \"condition\" not in df.columns:\n",
    "#         raise ValueError(f\"{name}: no 'condition' column found.\")\n",
    "#     if \"time_sec_rel\" not in df.columns:\n",
    "#         raise ValueError(f\"{name}: no 'time_sec_rel' column found.\")\n",
    "    \n",
    "#     print(pd.unique(df['condition']))\n",
    "\n",
    "#     d = df[df[\"condition\"] == \"post_chat\"].copy()\n",
    "#     # d.to_csv('post_chat_only.csv')\n",
    "#     print(\"all cols + post chat values\", d.shape)\n",
    "#     hbo_cols = get_hbo_columns(d)\n",
    "#     if len(hbo_cols) == 0:\n",
    "#         raise ValueError(f\"{name}: no HbO columns found after filtering to post_chat.\")\n",
    "    \n",
    "#     X = d[hbo_cols].astype(float).values\n",
    "#     print(X.shape)\n",
    "    \n",
    "#     duration_sec = len(d)/10.0\n",
    "#     duration_min = duration_sec / 60.0\n",
    "#     # med_dt, hz = estimate_sampling_rate(t)\n",
    "#     miss = missingness_rate(X)\n",
    "    \n",
    "#     print(f\"\\n=== EDA (post_chat, HbO only): {name} ===\")\n",
    "#     print(\"Rows (time points):\", len(d))\n",
    "#     print(\"HbO channels (D):\", len(hbo_cols))\n",
    "#     print(f\"Duration: {duration_sec:.2f} sec (~{duration_min:.2f} min)\")\n",
    "#     # print(f\"Estimated sampling: dt≈{med_dt:.4f}s  ->  {hz:.2f} Hz\")\n",
    "#     print(f\"Missingness rate (NaN fraction): {miss:.6f}\")\n",
    "\n",
    "#     print(f\"Total rows in file: {len(df_a)}\")\n",
    "#     print(f\"Post_chat rows: {len(d)}\")\n",
    "#     print(f\"Percentage: {100 * len(d) / len(df_a):.1f}%\")\n",
    "    \n",
    "#     return d, hbo_cols, X, d['time_sec']\n",
    "\n",
    "# a,b,c,d = eda_postchat_hbo(df_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2856ed",
   "metadata": {},
   "source": [
    "### Verifying matching dyad existance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c325e216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total child files: 44\n",
      "Total parent files: 44\n",
      "Matched dyads (have both child & parent): 44\n",
      "\n",
      "Child files with 'post_chat': 41\n",
      "Parent files with 'post_chat': 41\n",
      "Complete pairs with 'post_chat' in both: 41\n",
      "\n",
      "=== Matched pairs with post_chat ===\n",
      "\n",
      "Dyad 004 (dyad_id: 4, match: True)\n",
      "  Child:  FINS_004c_aligned_timeseries_filtered.csv\n",
      "  Parent: FINS_004p_aligned_timeseries_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory path\n",
    "data_dir = Path(\"../aligned_timeseries_FINS\")\n",
    "\n",
    "# Get all CSV files matching child and parent patterns\n",
    "child_files = sorted(data_dir.glob(\"FINS_*c_*_filtered.csv\"))\n",
    "parent_files = sorted(data_dir.glob(\"FINS_*p_*_filtered.csv\"))\n",
    "\n",
    "# Create dictionaries to store dyad information\n",
    "child_dyads = {}  # {dyad_number: filename}\n",
    "parent_dyads = {}  # {dyad_number: filename}\n",
    "\n",
    "# Extract dyad numbers from filenames\n",
    "for file_path in child_files:\n",
    "    # Extract dyad number (e.g., \"040\" from \"FINS_040c_aligned_timeseries_filtered.csv\")\n",
    "    dyad_num = file_path.stem.split('_')[1].replace('c', '')\n",
    "    child_dyads[dyad_num] = file_path.name\n",
    "\n",
    "for file_path in parent_files:\n",
    "    # Extract dyad number (e.g., \"040\" from \"FINS_040p_aligned_timeseries_filtered.csv\")\n",
    "    dyad_num = file_path.stem.split('_')[1].replace('p', '')\n",
    "    parent_dyads[dyad_num] = file_path.name\n",
    "\n",
    "# Find dyads that have both child and parent files\n",
    "matched_dyads = set(child_dyads.keys()).intersection(set(parent_dyads.keys()))\n",
    "# print(sorted(matched_dyads))\n",
    "\n",
    "# Lists to store files with post_chat (for matched dyads only)\n",
    "child_files_with_post_chat = []\n",
    "parent_files_with_post_chat = []\n",
    "matched_pairs = []\n",
    "\n",
    "# Check matched dyads for post_chat condition\n",
    "for dyad_num in sorted(matched_dyads):\n",
    "    child_file = data_dir / child_dyads[dyad_num]\n",
    "    parent_file = data_dir / parent_dyads[dyad_num]\n",
    "    \n",
    "    child_has_post_chat = False\n",
    "    parent_has_post_chat = False\n",
    "    child_dyad_id = None\n",
    "    parent_dyad_id = None\n",
    "    \n",
    "    try:\n",
    "        # Check child file\n",
    "        df_child = pd.read_csv(child_file)\n",
    "        if 'condition' in df_child.columns and 'post_chat' in df_child['condition'].values:\n",
    "            child_has_post_chat = True\n",
    "            child_files_with_post_chat.append(child_dyads[dyad_num])\n",
    "            if 'dyad_id' in df_child.columns:\n",
    "                child_dyad_id = df_child['dyad_id'].iloc[0]\n",
    "        \n",
    "        # Check parent file\n",
    "        df_parent = pd.read_csv(parent_file)\n",
    "        if 'condition' in df_parent.columns and 'post_chat' in df_parent['condition'].values:\n",
    "            parent_has_post_chat = True\n",
    "            parent_files_with_post_chat.append(parent_dyads[dyad_num])\n",
    "            if 'dyad_id' in df_parent.columns:\n",
    "                parent_dyad_id = df_parent['dyad_id'].iloc[0]\n",
    "        \n",
    "        # Verify dyad_id matches\n",
    "        dyad_id_match = (child_dyad_id == parent_dyad_id) if (child_dyad_id and parent_dyad_id) else \"N/A\"\n",
    "        \n",
    "        # If both have post_chat, add to matched pairs\n",
    "        if child_has_post_chat and parent_has_post_chat:\n",
    "            matched_pairs.append({\n",
    "                'dyad_num': dyad_num,\n",
    "                'child_file': child_dyads[dyad_num],\n",
    "                'parent_file': parent_dyads[dyad_num],\n",
    "                'dyad_id_match': dyad_id_match,\n",
    "                'dyad_id': child_dyad_id if child_dyad_id else parent_dyad_id\n",
    "            })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading dyad {dyad_num}: {e}\")\n",
    "\n",
    "# Print results\n",
    "print(f\"Total child files: {len(child_files)}\")\n",
    "print(f\"Total parent files: {len(parent_files)}\")\n",
    "print(f\"Matched dyads (have both child & parent): {len(matched_dyads)}\")\n",
    "print(f\"\\nChild files with 'post_chat': {len(child_files_with_post_chat)}\")\n",
    "print(f\"Parent files with 'post_chat': {len(parent_files_with_post_chat)}\")\n",
    "print(f\"Complete pairs with 'post_chat' in both: {len(matched_pairs)}\")\n",
    "\n",
    "print(\"\\n=== Matched pairs with post_chat ===\")\n",
    "for pair in matched_pairs:\n",
    "    print(f\"\\nDyad {pair['dyad_num']} (dyad_id: {pair['dyad_id']}, match: {pair['dyad_id_match']})\")\n",
    "    print(f\"  Child:  {pair['child_file']}\")\n",
    "    print(f\"  Parent: {pair['parent_file']}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c34f4e",
   "metadata": {},
   "source": [
    "### Finding condition lenghts for each child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee360630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "022c: 79.7 secs 797 rows\n",
      "053c: 76.8 secs 768 rows\n",
      "015c: 74.0 secs 740 rows\n",
      "011c: 122.0 secs 1220 rows\n",
      "057c: 68.8 secs 688 rows\n",
      "039c: 72.6 secs 726 rows\n",
      "048c: 61.0 secs 610 rows\n",
      "019c: 122.0 secs 1220 rows\n",
      "006c: 71.8 secs 718 rows\n",
      "040c: 61.0 secs 610 rows\n",
      "044c: 65.4 secs 654 rows\n",
      "043c: 61.0 secs 610 rows\n",
      "032c: 80.7 secs 807 rows\n",
      "029c: 11.1 secs 111 rows\n",
      "036c: 78.7 secs 787 rows\n",
      "021c: 61.0 secs 610 rows\n",
      "016c: 61.0 secs 610 rows\n",
      "050c: 66.6 secs 666 rows\n",
      "009c: 94.7 secs 947 rows\n",
      "025c: 80.1 secs 801 rows\n",
      "054c: 68.3 secs 683 rows\n",
      "045c: 67.0 secs 670 rows\n",
      "018c: 61.0 secs 610 rows\n",
      "007c: 73.6 secs 736 rows\n",
      "056c: 79.9 secs 799 rows\n",
      "010c: 61.0 secs 610 rows\n",
      "027c: 72.7 secs 727 rows\n",
      "049c: 69.4 secs 694 rows\n",
      "038c: 75.8 secs 758 rows\n",
      "014c: 122.0 secs 1220 rows\n",
      "052c: 32.8 secs 328 rows\n",
      "023c: 73.6 secs 736 rows\n",
      "013c: 122.0 secs 1220 rows\n",
      "055c: 70.3 secs 703 rows\n",
      "024c: 69.4 secs 694 rows\n",
      "051c: 65.9 secs 659 rows\n",
      "017c: 71.2 secs 712 rows\n",
      "008c: 122.0 secs 1220 rows\n",
      "037c: 72.8 secs 728 rows\n",
      "033c: 84.2 secs 842 rows\n",
      "004c: 67.1 secs 671 rows\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"../aligned_timeseries_FINS\")\n",
    "\n",
    "# Get all CSV files matching the pattern\n",
    "matching_files = [f for f in data_dir.glob(\"FINS_*c_*_filtered.csv\")]\n",
    "\n",
    "# List to store files with post_chat\n",
    "files_with_post_chat = []\n",
    "\n",
    "# Check each file for post_chat in condition column\n",
    "for file_path in matching_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'condition' in df.columns:\n",
    "            if 'post_chat' in df['condition'].values:\n",
    "                pc_df = df[df['condition'] == 'post_chat']\n",
    "                duration = len(pc_df)/10.0\n",
    "                print(f\"{file_path.name.split('_')[1]}: {duration} secs {len(pc_df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2aa927a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4295\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['pre_chat', 'easy_talking', 'easy_silence', 'med_talking',\n",
       "       'med_silence', 'hard_talking', 'hard_silence', 'post_chat'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/Users/ineshtandon/Documents/GitHub/REN_fNIRS_State_Analysis/aligned_timeseries_FINS/FINS_029c_aligned_timeseries_filtered.csv')\n",
    "print(len(df))\n",
    "pd.unique(df['condition'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_mode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
